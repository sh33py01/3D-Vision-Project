{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp39-cp39-win_amd64.whl (216.0 MB)\n",
      "     -------------------------------------- 216.0/216.0 MB 4.0 MB/s eta 0:00:00\n",
      "Collecting open3d\n",
      "  Downloading open3d-0.19.0-cp39-cp39-win_amd64.whl (69.5 MB)\n",
      "     ---------------------------------------- 69.5/69.5 MB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.22.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.5.2)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 5.7 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "     -------------------------------------- 199.1/199.1 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.10.0\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 43.8/43.8 kB ? eta 0:00:00\n",
      "Collecting flask>=3.0.0\n",
      "  Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "     -------------------------------------- 103.3/103.3 kB 5.8 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=3.0.0\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "     -------------------------------------- 224.5/224.5 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting dash>=2.6.0\n",
      "  Downloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n",
      "     ---------------------------------------- 7.9/7.9 MB 7.0 MB/s eta 0:00:00\n",
      "Collecting ipywidgets>=8.0.4\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.8/139.8 kB 8.6 MB/s eta 0:00:00\n",
      "Collecting configargparse\n",
      "  Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n",
      "Collecting nbformat>=5.7.0\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dash>=2.6.0->open3d) (2.27.1)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dash>=2.6.0->open3d) (1.5.5)\n",
      "Collecting retrying\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Collecting werkzeug>=3.0.0\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "     -------------------------------------- 228.0/228.0 kB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dash>=2.6.0->open3d) (58.1.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from dash>=2.6.0->open3d) (4.11.4)\n",
      "Collecting flask>=3.0.0\n",
      "  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "     -------------------------------------- 101.7/101.7 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting plotly>=5.0.0\n",
      "  Downloading plotly-6.1.2-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 16.3/16.3 MB 5.9 MB/s eta 0:00:00\n",
      "Collecting blinker>=1.6.2\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flask>=3.0.0->open3d) (8.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flask>=3.0.0->open3d) (2.1.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipywidgets>=8.0.4->open3d) (5.2.2.post1)\n",
      "Collecting widgetsnbextension~=4.0.14\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 9.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipywidgets>=8.0.4->open3d) (8.4.0)\n",
      "Collecting jupyterlab_widgets~=3.0.15\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "     -------------------------------------- 216.6/216.6 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting comm>=0.1.3\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Collecting jsonschema>=2.6\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "     ---------------------------------------- 88.7/88.7 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting jupyter-core!=5.0.*,>=4.12\n",
      "  Downloading jupyter_core-5.8.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     -------------------------------------- 536.2/536.2 kB 8.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.29)\n",
      "Requirement already satisfied: stack-data in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.12.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Collecting attrs>=22.2.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 63.8/63.8 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.25.1-cp39-cp39-win_amd64.whl (231 kB)\n",
      "     -------------------------------------- 231.5/231.5 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting traitlets>=4.3.1\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "     ---------------------------------------- 85.4/85.4 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting platformdirs>=2.5\n",
      "  Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (304)\n",
      "Collecting narwhals>=1.15.1\n",
      "  Downloading narwhals-1.42.0-py3-none-any.whl (359 kB)\n",
      "     ------------------------------------- 359.0/359.0 kB 21.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->dash>=2.6.0->open3d) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->dash>=2.6.0->open3d) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->dash>=2.6.0->open3d) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->dash>=2.6.0->open3d) (1.26.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ertat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.2)\n",
      "Installing collected packages: mpmath, fastjsonschema, widgetsnbextension, werkzeug, typing-extensions, traitlets, sympy, rpds-py, retrying, platformdirs, networkx, narwhals, jupyterlab_widgets, fsspec, filelock, configargparse, blinker, attrs, torch, referencing, plotly, jupyter-core, flask, comm, jsonschema-specifications, dash, jsonschema, ipywidgets, nbformat, open3d\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.2.0\n",
      "    Uninstalling typing_extensions-4.2.0:\n",
      "      Successfully uninstalled typing_extensions-4.2.0\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.2.2.post1\n",
      "    Uninstalling traitlets-5.2.2.post1:\n",
      "      Successfully uninstalled traitlets-5.2.2.post1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.1.0\n",
      "    Uninstalling attrs-22.1.0:\n",
      "      Successfully uninstalled attrs-22.1.0\n",
      "  Attempting uninstall: jupyter-core\n",
      "    Found existing installation: jupyter-core 4.10.0\n",
      "    Uninstalling jupyter-core-4.10.0:\n",
      "      Successfully uninstalled jupyter-core-4.10.0\n",
      "  Attempting uninstall: flask\n",
      "    Found existing installation: Flask 2.1.2\n",
      "    Uninstalling Flask-2.1.2:\n",
      "      Successfully uninstalled Flask-2.1.2\n",
      "Successfully installed attrs-25.3.0 blinker-1.9.0 comm-0.2.2 configargparse-1.7.1 dash-3.0.4 fastjsonschema-2.21.1 filelock-3.18.0 flask-3.0.3 fsspec-2025.5.1 ipywidgets-8.1.7 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 jupyter-core-5.8.1 jupyterlab_widgets-3.0.15 mpmath-1.3.0 narwhals-1.42.0 nbformat-5.10.4 networkx-3.2.1 open3d-0.19.0 platformdirs-4.3.8 plotly-6.1.2 referencing-0.36.2 retrying-1.3.4 rpds-py-0.25.1 sympy-1.14.0 torch-2.7.1 traitlets-5.14.3 typing-extensions-4.14.0 werkzeug-3.0.6 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch open3d tqdm numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare ShapeNet Data:\n",
    "Download ShapeNet point clouds or convert from meshes using tools like trimesh\n",
    "Save each point cloud as a .npy file in a structured directory\n",
    "\n",
    "Training:\n",
    "Update \"path_to_shapenet_point_clouds\" in the dataset initialization\n",
    "Run train_point_cloud_gan()\n",
    "Monitor losses and visualize generated samples\n",
    "\n",
    "Generation:\n",
    "After training, load the generator:\n",
    "////\n",
    "generator = Generator().to(device)\n",
    "generator.load_state_dict(torch.load(\"generator.pth\"))\n",
    "Generate new point clouds:\n",
    "///\n",
    "z = torch.randn(1, latent_dim).to(device)\n",
    "new_pc = generator(z).cpu().numpy()[0]\n",
    "visualize_point_cloud(new_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_points=2048, split='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the point clouds (.npy files)\n",
    "            num_points (int): Number of points to sample from each cloud\n",
    "            split (str): 'train' or 'test'\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.num_points = num_points\n",
    "        self.files = []\n",
    "        \n",
    "        for class_dir in os.listdir(root_dir):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file in os.listdir(class_path):\n",
    "                    if file.endswith('.npy'):\n",
    "                        self.files.append(os.path.join(class_path, file))\n",
    "        \n",
    "        split_idx = int(0.8 * len(self.files))\n",
    "        self.files = self.files[:split_idx] if split == 'train' else self.files[split_idx:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pc = np.load(self.files[idx])  # Shape: (N, 3)\n",
    "        \n",
    "        if pc.shape[0] > self.num_points:\n",
    "            idxs = np.random.choice(pc.shape[0], self.num_points, replace=False)\n",
    "            pc = pc[idxs]\n",
    "        elif pc.shape[0] < self.num_points:\n",
    "            idxs = np.random.choice(pc.shape[0], self.num_points, replace=True)\n",
    "            pc = pc[idxs]\n",
    "        \n",
    "        pc = torch.from_numpy(pc).float()\n",
    "        pc = self.normalize_point_cloud(pc)\n",
    "        return pc\n",
    "\n",
    "    def normalize_point_cloud(self, pc):\n",
    "        \"\"\" Normalize point cloud to fit in [-1, 1] cube \"\"\"\n",
    "        centroid = torch.mean(pc, dim=0)\n",
    "        pc = pc - centroid\n",
    "        max_dist = torch.max(torch.sqrt(torch.sum(pc**2, dim=1)))\n",
    "        pc = pc / max_dist\n",
    "        return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_points=2048):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(1024, num_points * 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        out = self.model(z)\n",
    "        out = out.view(batch_size, self.num_points, 3)  # (batch_size, num_points, 3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_points=2048):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        \n",
    "        # Shared MLP layers (similar to PointNet)\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Global feature layer\n",
    "        self.global_conv = nn.Conv1d(256, 512, 1)\n",
    "        self.global_bn = nn.BatchNorm1d(512)\n",
    "        \n",
    "        # Classification layers\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, num_points, 3)\n",
    "        x = x.transpose(2, 1)  # (batch_size, 3, num_points)\n",
    "        \n",
    "        # Shared MLP\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Global features\n",
    "        x = self.relu(self.global_bn(self.global_conv(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 512)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
    "    \"\"\" Calculates the gradient penalty loss for WGAN-GP \"\"\"\n",
    "    batch_size = real_samples.size(0)\n",
    "    # Random weight term for interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1).to(device)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    fake = torch.ones(batch_size, 1).to(device)\n",
    "    \n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def train_point_cloud_gan():\n",
    "    # Hyperparameters\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    latent_dim = 128\n",
    "    num_points = 2048\n",
    "    batch_size = 32\n",
    "    epochs = 200\n",
    "    lr = 0.0001\n",
    "    n_critic = 5  # Number of discriminator updates per generator update\n",
    "    lambda_gp = 10  # Gradient penalty coefficient\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = PointCloudDataset(\"path_to_shapenet_point_clouds\", num_points=num_points)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize networks\n",
    "    generator = Generator(latent_dim, num_points).to(device)\n",
    "    discriminator = Discriminator(num_points).to(device)\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for i, real_pcs in enumerate(tqdm(dataloader)):\n",
    "            real_pcs = real_pcs.to(device)\n",
    "            batch_size = real_pcs.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Generate fake point clouds\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_pcs = generator(z)\n",
    "            \n",
    "            # Real and fake losses\n",
    "            real_loss = -torch.mean(discriminator(real_pcs))\n",
    "            fake_loss = torch.mean(discriminator(fake_pcs.detach()))\n",
    "            \n",
    "            # Gradient penalty\n",
    "            gp = gradient_penalty(discriminator, real_pcs.data, fake_pcs.data, device)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = real_loss + fake_loss + lambda_gp * gp\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Train Generator every n_critic steps\n",
    "            if i % n_critic == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "                \n",
    "                # Generate fake point clouds\n",
    "                z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                fake_pcs = generator(z)\n",
    "                \n",
    "                # Generator loss\n",
    "                g_loss = -torch.mean(discriminator(fake_pcs))\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "            \n",
    "        # Print progress\n",
    "        print(f\"[Epoch {epoch}/{epochs}] D loss: {d_loss.item():.4f}, G loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        # Save generated samples periodically\n",
    "        if epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                sample_z = torch.randn(1, latent_dim).to(device)\n",
    "                sample_pc = generator(sample_z).cpu().numpy()\n",
    "                save_point_cloud(sample_pc[0], f\"generated_samples/sample_{epoch}.ply\")\n",
    "    \n",
    "    # Save models\n",
    "    torch.save(generator.state_dict(), \"generator.pth\")\n",
    "    torch.save(discriminator.state_dict(), \"discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "def save_point_cloud(points, filename):\n",
    "    \"\"\" Save point cloud to PLY file \"\"\"\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    o3d.io.write_point_cloud(filename, pcd)\n",
    "\n",
    "def visualize_point_cloud(points):\n",
    "    \"\"\" Interactive visualization \"\"\"\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
